{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHKJfUUh_B29",
        "outputId": "a8f2f5fc-f195-4262-bd3f-932999aff67c"
      },
      "id": "fHKJfUUh_B29",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd \"drive/My Drive/GenSentiment\""
      ],
      "metadata": {
        "id": "SdEKMj_X_NZ8"
      },
      "id": "SdEKMj_X_NZ8",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pulling in github\n",
        "# import os\n",
        "# import getpass\n",
        "# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "# !sudo apt-get install git-lfs\n",
        "# !git lfs install\n",
        "\n",
        "# # Remove Colab default sample_data\n",
        "# !rm -r ./sample_data\n",
        "\n",
        "# # Clone GitHub files to colab workspace\n",
        "# git_user = \"Cloblak\" # Enter user or organization name\n",
        "# git_token = \"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\" # Enter your email\n",
        "# repo_name = \"Cloblak/GeneralizingSentiment\" # Enter repo name\n",
        "# git_path = f\"https://{git_user}:{git_token}@github.com/{repo_name}.git\"\n",
        "# !git clone \"{git_path}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w_fwEpl8y63",
        "outputId": "08c13bfa-b94a-4c4b-eeaf-90445049fb75"
      },
      "id": "6w_fwEpl8y63",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected operating system as Ubuntu/bionic.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.1.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 52 not upgraded.\n",
            "Git LFS initialized.\n",
            "rm: cannot remove './sample_data': No such file or directory\n",
            "Cloning into 'GeneralizingSentiment'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 163 (delta 67), reused 47 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 3.75 MiB | 11.22 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n",
            "Filtering content: 100% (2/2), 1.03 GiB | 37.03 MiB/s, done.\n",
            "fatal: cannot exec '.git/hooks/post-checkout': Permission denied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"drive/MyDrive/GenSentiment\")"
      ],
      "metadata": {
        "id": "yUw-nT6A9ocH"
      },
      "id": "yUw-nT6A9ocH",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "17ee606c-6536-4322-a454-c5a93af17a7c",
      "metadata": {
        "id": "17ee606c-6536-4322-a454-c5a93af17a7c"
      },
      "outputs": [],
      "source": [
        "# Term Frequency-Inverse Document Frequency (TFIDF) Attempt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "160ef1f6-1617-4878-b66c-037fc7c2e19a",
      "metadata": {
        "id": "160ef1f6-1617-4878-b66c-037fc7c2e19a",
        "outputId": "0a572e84-b513-40e3-e1fa-72d4a656d80c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# import packages\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import randint\n",
        "import seaborn as sns # used for plot interactive graph.\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from IPython.display import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YlJV0-20DZb0",
        "outputId": "5be3181a-1c44-443a-93e3-983d03dd7286"
      },
      "id": "YlJV0-20DZb0",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/GenSentiment/GeneralizingSentiment'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"GeneralizingSentiment\")\n",
        "from scripts import data_preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDRCk8jhBp96",
        "outputId": "177de7bf-3c95-4c6e-c019-2a091821a279"
      },
      "id": "UDRCk8jhBp96",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9JcJhbGnAKjU",
        "outputId": "baa0fab8-e148-4f66-afa9-0a87caf942c7"
      },
      "id": "9JcJhbGnAKjU",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/GenSentiment/GeneralizingSentiment'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f3cfd1a7-4c64-4b9f-97c3-38e2fce943cf",
      "metadata": {
        "id": "f3cfd1a7-4c64-4b9f-97c3-38e2fce943cf"
      },
      "outputs": [],
      "source": [
        "# import data\n",
        "\n",
        "labeled_df = pd.read_parquet(\"data/data_prepipeline/full_raw_data.parquet.gzip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "608aca50-d32a-489d-b26d-ceedd6442baf",
      "metadata": {
        "id": "608aca50-d32a-489d-b26d-ceedd6442baf"
      },
      "outputs": [],
      "source": [
        "labeled_df = labeled_df.sample(25000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b# clean and undersample to balance sentiment classes\n",
        "cleaned_df, tfidif_matrix = data_preprocessing.preprocessing(labeled_df['text'])\n",
        "labeled_df[\"text_cleaned\"] = cleaned_df\n",
        "\n",
        "labeled_df['sentiment_id'] = labeled_df['sentiment']\n",
        "labeled_df['sentiment_id'] = labeled_df['sentiment_id'].replace(\"negative\", 0)\n",
        "labeled_df['sentiment_id'] = labeled_df['sentiment_id'].replace(\"neutral\", 1)\n",
        "labeled_df['sentiment_id'] = labeled_df['sentiment_id'].replace(\"positive\", 2)\n",
        "labeled_df['sentiment_id'].value_counts()\n",
        "\n",
        "# Undersample to handle unbalanced dataset\n",
        "\n",
        "# Class count\n",
        "count_class_2, count_class_1, count_class_0 = labeled_df.sentiment_id.value_counts()\n",
        "\n",
        "print(f'Postive Class Count {count_class_2}')\n",
        "print(f'Neutral Class Count {count_class_1}')\n",
        "print(f'Negative Class Count {count_class_0}')\n",
        "\n",
        "df_class_0 = labeled_df[labeled_df['sentiment_id'] == 0]\n",
        "df_class_1 = labeled_df[labeled_df['sentiment_id'] == 1]\n",
        "df_class_2 = labeled_df[labeled_df['sentiment_id'] == 2]\n",
        "\n",
        "df_class_1_under = df_class_1.sample(count_class_0)\n",
        "df_class_2_under = df_class_2.sample(count_class_0)\n",
        "undersample_df_cleaned = pd.concat([df_class_0, df_class_1_under, df_class_2_under], axis=0)\n",
        "\n",
        "print('Random under-sampling:')\n",
        "print(undersample_df_cleaned.sentiment.value_counts())\n",
        "\n",
        "undersample_df_cleaned.sentiment.value_counts().plot(kind='bar', title='Count (target)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "15laS1Wy6Dyh",
        "outputId": "fe16b251-7a51-455d-b8e1-4962f1d1395a"
      },
      "id": "15laS1Wy6Dyh",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Postive Class Count 18489\n",
            "Neutral Class Count 4147\n",
            "Negative Class Count 2364\n",
            "Random under-sampling:\n",
            "negative    2364\n",
            "neutral     2364\n",
            "positive    2364\n",
            "Name: sentiment, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa72d8ed390>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Word Sentiment Classification"
      ],
      "metadata": {
        "id": "2kr_jQ0K-wUl"
      },
      "id": "2kr_jQ0K-wUl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu8LmDvQ_LnV",
        "outputId": "579951d7-cb24-4cff-99eb-b00582a39b8e"
      },
      "id": "mu8LmDvQ_LnV",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 34.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.63.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 64.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.10.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.6.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 67.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 64.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 67.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.11.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=625092bbbf6dc430793ac50388f5c7c7ed47adf51c2780961cea55d38e5fcc3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/noneDL_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJAGP2UC-wjl",
        "outputId": "aff6cf0d-4c46-4118-d169-f2ab680e8798"
      },
      "id": "OJAGP2UC-wjl",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "Use a Subsample of the Data Set (y/n):y\n",
            "Modeling Data With None DL Approach: Logistic Regression...\n",
            "Postive Class Count 55443\n",
            "Neutral Class Count 12667\n",
            "Negative Class Count 6890\n",
            "Random under-sampling:\n",
            "negative    6890\n",
            "neutral     6890\n",
            "positive    6890\n",
            "Name: sentiment, dtype: int64\n",
            "Accuracy on the training set is 0.713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Words"
      ],
      "metadata": {
        "id": "dmcPflrK-wpn"
      },
      "id": "dmcPflrK-wpn",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = undersample_df_cleaned.text_cleaned\n",
        "y = undersample_df_cleaned.sentiment_id\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size = 0.25,train_size =0.75)"
      ],
      "metadata": {
        "id": "JSGQ1aZEOOdZ"
      },
      "id": "JSGQ1aZEOOdZ",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as n\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFQfNFtdOOkH",
        "outputId": "fb2152ab-3c79-468d-a451-3c716a6290b7"
      },
      "id": "qFQfNFtdOOkH",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "REPLACE_IP_ADDRESS = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
        "\n",
        "def text_prepare(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = text.replace('\\n', ' ').lower()# lowercase text\n",
        "    text = REPLACE_IP_ADDRESS.sub('', text)\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ',text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('',text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join([w for w in text.split() if not w in STOPWORDS])# delete stopwords from text\n",
        "    return text"
      ],
      "metadata": {
        "id": "XDRS-gosOOnd"
      },
      "id": "XDRS-gosOOnd",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary of all words from train corpus with their counts.\n",
        "words_counts = {}\n",
        "for comments in X_train:\n",
        "    for word in comments.split():\n",
        "        if word not in words_counts:\n",
        "            words_counts[word] = 1\n",
        "        words_counts[word] += 1\n",
        "        \n",
        "DICT_SIZE = 10000\n",
        "POPULAR_WORDS = sorted(words_counts, key=words_counts.get, reverse=True)[:DICT_SIZE]\n",
        "WORDS_TO_INDEX = {key: rank for rank, key in enumerate(POPULAR_WORDS, 0)}\n",
        "INDEX_TO_WORDS = {index:word for word, index in WORDS_TO_INDEX.items()}\n",
        "ALL_WORDS = WORDS_TO_INDEX.keys()"
      ],
      "metadata": {
        "id": "4jVut_NuOOqp"
      },
      "id": "4jVut_NuOOqp",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "POPULAR_WORDS[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lmbUseE-wtu",
        "outputId": "363f397f-c483-4aa3-9f6b-a8e8d76c6f77"
      },
      "id": "6lmbUseE-wtu",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'and', 'a', 'of', 'to', 'is', 'in', 'i', 'this', 'it']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse as sp_sparse"
      ],
      "metadata": {
        "id": "xYH0De4I-ww2"
      },
      "id": "xYH0De4I-ww2",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_bag_of_words(text, words_to_index, dict_size):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        dict_size: size of the dictionary\n",
        "        \n",
        "        return a vector which is a bag-of-words representation of 'text'\n",
        "    \"\"\"\n",
        "    result_vector = np.zeros(dict_size)\n",
        "    for word in text.split(' '):\n",
        "        if word in words_to_index:\n",
        "            result_vector[words_to_index[word]] +=1\n",
        "    return result_vector\n",
        "\n",
        "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
        "X_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_val])\n",
        "print('X_train shape ', X_train_mybag.shape, '\\nX_val shape ', X_val_mybag.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTbm5kC8PWWq",
        "outputId": "1ec055f3-fa68-4e97-c0f4-84bfe146a77d"
      },
      "id": "ZTbm5kC8PWWq",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape  (4254, 10000) \n",
            "X_val shape  (1419, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "pXR3lRNUPWZw"
      },
      "id": "pXR3lRNUPWZw",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf_features(X_train, X_val, X_test):\n",
        "    \"\"\"\n",
        "        X_train, X_test — samples        \n",
        "        return TF-IDF vectorized representation of each sample and vocabulary\n",
        "    \"\"\"\n",
        "    # Create TF-IDF vectorizer with a proper parameters choice\n",
        "    # Fit the vectorizer on the train set\n",
        "    # Transform the train, test set and return the result\n",
        "    \n",
        "    \n",
        "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=5, token_pattern='(\\S+)')\n",
        "\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "    X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "    \n",
        "    return X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vectorizer.vocabulary_\n",
        "\n",
        "X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_val, X_test)\n",
        "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}"
      ],
      "metadata": {
        "id": "5XaZC4owPWcx"
      },
      "id": "5XaZC4owPWcx",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "deuajy-JPbPx"
      },
      "id": "deuajy-JPbPx",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(X_train, y_train, C, regularisation):\n",
        "    \"\"\"\n",
        "      X_train, y_train — training data\n",
        "      \n",
        "      return: trained classifier\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create and fit LogisticRegression wraped into OneVsRestClassifier.\n",
        "\n",
        "    model = OneVsRestClassifier(LogisticRegression(penalty=regularisation, C=C, max_iter=10000)).fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "classifier_mybag = train_classifier(X_train_mybag, y_train, C = 4, regularisation = 'l2')\n",
        "classifier_tfidf = train_classifier(X_train_tfidf, y_train, C = 4, regularisation = 'l2')\n",
        "\n",
        "y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\n",
        "y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\n",
        "y_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\n",
        "y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)"
      ],
      "metadata": {
        "id": "jvTi0CHhPbVx"
      },
      "id": "jvTi0CHhPbVx",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score \n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import recall_score"
      ],
      "metadata": {
        "id": "UfksqPJjPbY4"
      },
      "id": "UfksqPJjPbY4",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_evaluation_scores(y_test, predicted):\n",
        "    \n",
        "    print('Accuracy: ', accuracy_score(y_test, predicted, normalize=False))\n",
        "    print('F1-score macro: ', f1_score(y_test, predicted, average='macro'))\n",
        "    print('F1-score micro: ', f1_score(y_test, predicted, average='micro'))\n",
        "    print('F1-score weighted: ', f1_score(y_test, predicted, average='weighted'))\n",
        "    # print('Precision macro: ', average_precision_score(y_test, predicted, average='macro'))\n",
        "    # print('Precision micro: ', average_precision_score(y_test, predicted, average='micro'))\n",
        "    # print('Precision weighted: ', average_precision_score(y_test, predicted, average='weighted'))\n",
        "    \n",
        "print('Bag-of-words\\n')\n",
        "print_evaluation_scores(y_val, y_val_predicted_labels_mybag)\n",
        "print('\\nTfidf\\n')\n",
        "print_evaluation_scores(y_val, y_val_predicted_labels_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQEyoiDpPbcU",
        "outputId": "943fcc4d-6cc9-436f-d19f-5bce526f10e5"
      },
      "id": "AQEyoiDpPbcU",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-words\n",
            "\n",
            "Accuracy:  977\n",
            "F1-score macro:  0.6891602483099298\n",
            "F1-score micro:  0.6885130373502466\n",
            "F1-score weighted:  0.6882930099738361\n",
            "\n",
            "Tfidf\n",
            "\n",
            "Accuracy:  1048\n",
            "F1-score macro:  0.7393025452337426\n",
            "F1-score micro:  0.7385482734319945\n",
            "F1-score weighted:  0.7382985447426273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S3Nru5KsPWfZ"
      },
      "id": "S3Nru5KsPWfZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PQdTtzLdPWiD"
      },
      "id": "PQdTtzLdPWiD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "ecac2acb-d054-42be-bab7-87c53a4335bd",
      "metadata": {
        "id": "ecac2acb-d054-42be-bab7-87c53a4335bd",
        "outputId": "a97e38d8-9b79-4cb9-df38-e33fc7b3087e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Each of the 6792 complaints is represented by 15401 features (TF-IDF score of unigrams and bigrams)\n"
          ]
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
        "                        ngram_range=(1, 2), \n",
        "                        stop_words='english')\n",
        "# We transform each complaint into a vector\n",
        "features = tfidf.fit_transform(undersample_df_cleaned.text_cleaned).toarray()\n",
        "labels = undersample_df_cleaned.sentiment_id\n",
        "print(\"Each of the %d complaints is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "ce10753f-868d-4337-a839-5ed842d27bc5",
      "metadata": {
        "id": "ce10753f-868d-4337-a839-5ed842d27bc5"
      },
      "outputs": [],
      "source": [
        "# Create a new column 'category_id' with encoded categories \n",
        "category_id_df = undersample_df_cleaned[['sentiment', 'sentiment_id']].drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "4ecdf285-d23c-4bd9-8613-fbeec37ad9d6",
      "metadata": {
        "id": "4ecdf285-d23c-4bd9-8613-fbeec37ad9d6",
        "outputId": "e15b446f-59f2-4651-f2c1-d47bccfa2005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['negative', 0],\n",
              "       ['neutral', 1],\n",
              "       ['positive', 2]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "category_id_df.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "d6bfe8b7-cbfa-4b8e-b268-61a5ed0c3d55",
      "metadata": {
        "id": "d6bfe8b7-cbfa-4b8e-b268-61a5ed0c3d55"
      },
      "outputs": [],
      "source": [
        "# Dictionaries for future use\n",
        "category_to_id = dict(category_id_df.values)\n",
        "id_to_category = dict(category_id_df[['sentiment_id', 'sentiment']].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "960426d2-8e22-460f-9f6b-1dabea857dc6",
      "metadata": {
        "id": "960426d2-8e22-460f-9f6b-1dabea857dc6",
        "outputId": "57837dbc-80a1-494f-a58c-40b5f813093e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n==> negative:\n",
            "  * Most Correlated Unigrams are: money, worst, waste\n",
            "  * Most Correlated Bigrams are: waste money, dont waste, waste time\n",
            "n==> neutral:\n",
            "  * Most Correlated Unigrams are: okay, ok, bit\n",
            "  * Most Correlated Bigrams are: just ok, movie ok, pretty good\n",
            "n==> positive:\n",
            "  * Most Correlated Unigrams are: loved, love, great\n",
            "  * Most Correlated Bigrams are: great movie, love movie, highly recommend\n"
          ]
        }
      ],
      "source": [
        "# Finding the three most correlated terms with each of the product categories\n",
        "N = 3\n",
        "for Product, category_id in sorted(category_to_id.items()):\n",
        "    features_chi2 = chi2(features, labels == category_id)\n",
        "    indices = np.argsort(features_chi2[0])\n",
        "    feature_names = np.array(tfidf.get_feature_names_out())[indices]\n",
        "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "    print(\"n==> %s:\" %(Product))\n",
        "    print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n",
        "    print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "e312be1a-d6a7-46bc-8043-5dcf88572fe1",
      "metadata": {
        "id": "e312be1a-d6a7-46bc-8043-5dcf88572fe1"
      },
      "outputs": [],
      "source": [
        "# build model\n",
        "\n",
        "X = undersample_df_cleaned['text_cleaned'] # Collection of documents\n",
        "y = undersample_df_cleaned['sentiment_id'] # Target or the labels we want to predict (i.e., the 13 different complaints of products)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "a2c1be2c-6c67-4062-bb7c-164c077ebfdb",
      "metadata": {
        "id": "a2c1be2c-6c67-4062-bb7c-164c077ebfdb"
      },
      "outputs": [],
      "source": [
        "# importing the relevant modules\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# vectorizing the sentences\n",
        "cv = CountVectorizer(binary = True) # implies that it indicates whether the word is present or not.\n",
        "\n",
        "cv.fit(X) # find all the unique words from the training set\n",
        "\n",
        "X_train_vec = cv.transform(X_train)\n",
        "X_test_vec = cv.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "73452b4d-95f3-4c4a-bac0-82138c5986f8",
      "metadata": {
        "id": "73452b4d-95f3-4c4a-bac0-82138c5986f8"
      },
      "outputs": [],
      "source": [
        "# importing the relevant modules\n",
        "import xgboost as xgb\n",
        "\n",
        "# creating a variable for the new train and test sets\n",
        "xgb_train = xgb.DMatrix(X_train_vec, y_train)\n",
        "xgb_test = xgb.DMatrix(X_test_vec, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "f86c40eb-6021-4ebf-8e32-4642aeae88c2",
      "metadata": {
        "id": "f86c40eb-6021-4ebf-8e32-4642aeae88c2",
        "outputId": "41bb4f85-018f-4b62-bd67-60c1a75c7088",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5\n",
            "F1 Weighted: 0.5\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Setting the Parameters of the Model\n",
        "param = {'eta': 1,\n",
        "         'max_depth': 50,\n",
        "         'num_class': 3,\n",
        "         'objective': 'multi:softmax'}\n",
        " \n",
        "# Training the Model\n",
        "xgb_model = xgb.train(param, xgb_train, num_boost_round = 30)\n",
        "# Predicting using the Model\n",
        "y_pred = xgb_model.predict(xgb_test)\n",
        "y_pred = np.where(np.array(y_pred) > 0.5, 1, 0) # converting them to 1/0’s\n",
        "# Evaluation of Model\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')                 \n",
        "print(f'F1 Weighted: {f1_score(y_test, y_pred, average=\"micro\")}')    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "71a794c1-f9ed-4a28-a7c7-7e486cf3fa1a",
      "metadata": {
        "id": "71a794c1-f9ed-4a28-a7c7-7e486cf3fa1a"
      },
      "outputs": [],
      "source": [
        "models = [\n",
        "    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n",
        "    LinearSVC(),\n",
        "    MultinomialNB(),\n",
        "    LogisticRegression(random_state=0),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "639a5fa5-0d07-4f9e-adb5-425b096e8b5a",
      "metadata": {
        "id": "639a5fa5-0d07-4f9e-adb5-425b096e8b5a"
      },
      "outputs": [],
      "source": [
        "# 5 Cross-validation\n",
        "CV = 5\n",
        "cv_df = pd.DataFrame(index=range(CV * len(models)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "273f5df1-c598-46d3-b163-6caf9bcb88ce",
      "metadata": {
        "id": "273f5df1-c598-46d3-b163-6caf9bcb88ce"
      },
      "outputs": [],
      "source": [
        "entries = []\n",
        "for model in models:\n",
        "    model_name = model.__class__.__name__\n",
        "    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
        "    for fold_idx, accuracy in enumerate(accuracies):\n",
        "        entries.append((model_name, fold_idx, accuracy))\n",
        "        \n",
        "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "e2c72e24-ff1c-4e33-b606-cfd55c300047",
      "metadata": {
        "id": "e2c72e24-ff1c-4e33-b606-cfd55c300047",
        "outputId": "31d653ff-ca30-48ea-ea0b-96128788b62a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        Mean Accuracy  Standard deviation\n",
              "model_name                                               \n",
              "LinearSVC                    0.702445            0.005104\n",
              "LogisticRegression           0.719965            0.005124\n",
              "MultinomialNB                0.703625            0.015800\n",
              "RandomForestClassifier       0.595113            0.019816"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-821b982e-f1ed-404c-be18-4d2411ff88c3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mean Accuracy</th>\n",
              "      <th>Standard deviation</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model_name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>LinearSVC</th>\n",
              "      <td>0.702445</td>\n",
              "      <td>0.005104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LogisticRegression</th>\n",
              "      <td>0.719965</td>\n",
              "      <td>0.005124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MultinomialNB</th>\n",
              "      <td>0.703625</td>\n",
              "      <td>0.015800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForestClassifier</th>\n",
              "      <td>0.595113</td>\n",
              "      <td>0.019816</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-821b982e-f1ed-404c-be18-4d2411ff88c3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-821b982e-f1ed-404c-be18-4d2411ff88c3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-821b982e-f1ed-404c-be18-4d2411ff88c3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
        "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
        "\n",
        "acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n",
        "          ignore_index=True)\n",
        "acc.columns = ['Mean Accuracy', 'Standard deviation']\n",
        "acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "299b7fbe-32cb-406a-b6b3-ced5226301bc",
      "metadata": {
        "id": "299b7fbe-32cb-406a-b6b3-ced5226301bc",
        "outputId": "4bf350cf-98eb-48ad-8696-0db0005493fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a90b5d27-8856-4aa2-ab2e-dc246e4bc069",
      "metadata": {
        "id": "a90b5d27-8856-4aa2-ab2e-dc246e4bc069",
        "outputId": "28f696e3-9045-4746-f542-523dfea7e08e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 30 folds for each of 500 candidates, totalling 15000 fits\n"
          ]
        }
      ],
      "source": [
        "# Hyper tune best results\n",
        "\n",
        "# random search logistic regression model on the sonar dataset\n",
        "from scipy.stats import loguniform\n",
        "from pandas import read_csv\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# build model\n",
        "\n",
        "X = undersample_df_cleaned['text_cleaned'] # Collection of documents\n",
        "y = undersample_df_cleaned['sentiment_id'] # Target or the labels we want to predict (i.e., the 13 different complaints of products)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state = 0)\n",
        "\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
        "                        ngram_range=(1, 2), \n",
        "                        stop_words='english')\n",
        "\n",
        "X_train_vec = tfidf.fit_transform(X_train).toarray()\n",
        "X_test_vec = tfidf.fit_transform(X_test).toarray()\n",
        "\n",
        "model = LogisticRegression()\n",
        "# define evaluation\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1, )\n",
        "# define search space\n",
        "space = dict()\n",
        "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
        "space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\n",
        "space['C'] = loguniform(1e-5, 100)\n",
        "# define search\n",
        "search = RandomizedSearchCV(model, space, n_iter=500, scoring='accuracy', n_jobs=-1, cv=cv, random_state=1, verbose=10)\n",
        "# execute search\n",
        "result = search.fit(X_train_vec, y_train)\n",
        "# summarize result\n",
        "print('Best Score: %s' % result.best_score_)\n",
        "print('Best Hyperparameters: %s' % result.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5b4be3f-1b5b-4d0a-b1f3-a7e6a90837f6",
      "metadata": {
        "id": "e5b4be3f-1b5b-4d0a-b1f3-a7e6a90837f6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "name": "NoneDL_Model.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}